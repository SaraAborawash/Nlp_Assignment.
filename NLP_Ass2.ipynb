{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb02Gcm-vzVr",
        "outputId": "1dce35ad-7e92-4301-9fb6-72c9c18d6e4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Generate a document***"
      ],
      "metadata": {
        "id": "4Kd1R6iFyAfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a document based on a given phrase\n",
        "def generate_document(phrase, num_sentences=5):\n",
        "    document = \"\"\n",
        "    for _ in range(num_sentences):\n",
        "        document += phrase + \". \"\n",
        "    return document\n",
        "\n",
        "# Phrases for different fields/topics\n",
        "phrases = {\n",
        "    \"technology\": \"Artificial intelligence is revolutionizing the tech industry\",\n",
        "    \"finance\": \"The stock market is influenced by various factors including economic indicators\",\n",
        "    \"healthcare\": \"Advancements in medical technology have improved patient care\",\n",
        "}\n",
        "\n",
        "# Number of documents to generate for each field\n",
        "num_documents_per_field = 3\n",
        "\n",
        "# Generate documents for each field\n",
        "documents = {}\n",
        "for field, phrase in phrases.items():\n",
        "    documents[field] = [generate_document(phrase) for _ in range(num_documents_per_field)]"
      ],
      "metadata": {
        "id": "kElHb3SAvv9k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data processing steps***"
      ],
      "metadata": {
        "id": "SVUU_PubyWYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fMFNlKH2hDNU"
      },
      "outputs": [],
      "source": [
        "def process_data(document):\n",
        "    # Clean data by removing symbols and non-alphanumeric characters\n",
        "    document = re.sub(r'[^\\w\\s]', '', document)\n",
        "    # Normalize data by converting to lowercase\n",
        "    document = document.lower()\n",
        "    # Tokenization: split the data into words\n",
        "    tokens = word_tokenize(document)\n",
        "\n",
        "    # Initialize Porter Stemmer for stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    # Perform stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in stemmed_words if word not in stop_words]\n",
        "\n",
        "    return filtered_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Get unique words***\n",
        "\n"
      ],
      "metadata": {
        "id": "wExEgtO8yjq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all documents\n",
        "all_docs = []\n",
        "for field, docs in documents.items():\n",
        "    all_docs.extend(docs)\n",
        "\n",
        "# Process documents\n",
        "processed_docs = [process_data(doc) for doc in all_docs]\n",
        "\n",
        "# Join processed docs into strings for TfidfVectorizer\n",
        "doc_strings = [' '.join(doc) for doc in processed_docs]\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print feature names and TF-IDF matrix\n",
        "print(\"Unique Words:\", feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwFpxzuyv_sF",
        "outputId": "e37c1a03-1b7d-4225-848f-905c86ef2567"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words: ['advanc' 'artifici' 'care' 'econom' 'factor' 'improv' 'includ' 'indic'\n",
            " 'industri' 'influenc' 'intellig' 'market' 'medic' 'patient' 'revolution'\n",
            " 'stock' 'tech' 'technolog' 'variou']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Calculate TF-IDF using Scikit-learn***"
      ],
      "metadata": {
        "id": "dGo1yHyqy7qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate TF-IDF using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_sklearn = tfidf_vectorizer.fit_transform(doc_strings)\n",
        "\n",
        "# Convert TF-IDF matrix to array for easier manipulation\n",
        "tfidf_sklearn = tfidf_sklearn.toarray()\n",
        "\n",
        "# Print TF-IDF from scikit-learn\n",
        "print(\"\\nTF-IDF from scikit-learn:\")\n",
        "print(np.round(tfidf_sklearn, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g8uL-W2y6ib",
        "outputId": "1eab9651-4e05-487f-a4d1-fc4e4ae29869"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF from scikit-learn:\n",
            "[[0.   0.45 0.   0.   0.   0.   0.   0.   0.45 0.   0.45 0.   0.   0.\n",
            "  0.45 0.   0.45 0.   0.  ]\n",
            " [0.   0.45 0.   0.   0.   0.   0.   0.   0.45 0.   0.45 0.   0.   0.\n",
            "  0.45 0.   0.45 0.   0.  ]\n",
            " [0.   0.45 0.   0.   0.   0.   0.   0.   0.45 0.   0.45 0.   0.   0.\n",
            "  0.45 0.   0.45 0.   0.  ]\n",
            " [0.   0.   0.   0.35 0.35 0.   0.35 0.35 0.   0.35 0.   0.35 0.   0.\n",
            "  0.   0.35 0.   0.   0.35]\n",
            " [0.   0.   0.   0.35 0.35 0.   0.35 0.35 0.   0.35 0.   0.35 0.   0.\n",
            "  0.   0.35 0.   0.   0.35]\n",
            " [0.   0.   0.   0.35 0.35 0.   0.35 0.35 0.   0.35 0.   0.35 0.   0.\n",
            "  0.   0.35 0.   0.   0.35]\n",
            " [0.41 0.   0.41 0.   0.   0.41 0.   0.   0.   0.   0.   0.   0.41 0.41\n",
            "  0.   0.   0.   0.41 0.  ]\n",
            " [0.41 0.   0.41 0.   0.   0.41 0.   0.   0.   0.   0.   0.   0.41 0.41\n",
            "  0.   0.   0.   0.41 0.  ]\n",
            " [0.41 0.   0.41 0.   0.   0.41 0.   0.   0.   0.   0.   0.   0.41 0.41\n",
            "  0.   0.   0.   0.41 0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate TF-IDF from Scratch**\n"
      ],
      "metadata": {
        "id": "T2mff7v7yxHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *1.Calculate term frequency (TF)*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "62FAxquPfCM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf(document):\n",
        "    word_counts = Counter(document)\n",
        "    total_words = len(document)\n",
        "    tf = {word: count / total_words for word, count in word_counts.items()}\n",
        "    return tf"
      ],
      "metadata": {
        "id": "g_5RE6Tle-jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *2.Calculate inverse document frequency (IDF)*"
      ],
      "metadata": {
        "id": "ANnXkHCgfg-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_idf(documents):\n",
        "    num_documents = len(documents)\n",
        "    idf = {}\n",
        "    all_words = set([word for document in documents for word in document])\n",
        "\n",
        "    for word in all_words:\n",
        "        num_documents_containing_word = sum([1 for document in documents if word in document])\n",
        "        idf[word] = np.log((1 + num_documents) / (1 + num_documents_containing_word)) + 1\n",
        "\n",
        "    return idf"
      ],
      "metadata": {
        "id": "in4zTBhlf1eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *3.Calculate TF-IDF*\n"
      ],
      "metadata": {
        "id": "59KOO8frf9Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(documents):\n",
        "    tfidf = []\n",
        "    idf = calculate_idf(documents)\n",
        "\n",
        "    for document in documents:\n",
        "        tf = calculate_tf(document)\n",
        "        doc_tfidf = {word: tf[word] * idf[word] for word in document}\n",
        "        tfidf.append(doc_tfidf)\n",
        "\n",
        "    return tfidf\n"
      ],
      "metadata": {
        "id": "MKCeWyY9f5JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *4.Normalize TF-IDF vectors*"
      ],
      "metadata": {
        "id": "9s9qDLdLgQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tfidf(tfidf):\n",
        "    normalized_tfidf = []\n",
        "    for doc in tfidf:\n",
        "        norm = np.linalg.norm(list(doc.values()))\n",
        "        if norm > 0:\n",
        "            doc_tfidf_normalized = {word: value / norm for word, value in doc.items()}\n",
        "            normalized_tfidf.append(doc_tfidf_normalized)\n",
        "    return normalized_tfidf\n"
      ],
      "metadata": {
        "id": "v0PktQJ-gMfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *5.Fit and transform documents using TFIDF From scratch*"
      ],
      "metadata": {
        "id": "JOwX7bk8glOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_custom_tfidf(documents):\n",
        "    tfidf = []\n",
        "    idf = {}\n",
        "    vocab = {}\n",
        "\n",
        "    doc_count = len(documents)\n",
        "\n",
        "    # Compute term frequencies and document frequencies for IDF\n",
        "    tf = []\n",
        "    for document in documents:\n",
        "        doc_tf = {}\n",
        "        for word in document:\n",
        "            doc_tf[word] = doc_tf.get(word, 0) + 1\n",
        "        for word in doc_tf:\n",
        "            doc_tf[word] = doc_tf[word] / len(document)\n",
        "            idf[word] = idf.get(word, 0) + 1\n",
        "        tf.append(doc_tf)\n",
        "\n",
        "    # Sort the vocabulary alphabetically and assign indices\n",
        "    sorted_vocab = sorted(idf.keys())\n",
        "    vocab = {word: idx for idx, word in enumerate(sorted_vocab)}\n",
        "\n",
        "    # Compute IDF using the sorted vocabulary\n",
        "    for word in idf:\n",
        "        idf[word] = np.log((1 + doc_count) / (1 + idf[word])) + 1\n",
        "\n",
        "    # Compute TF-IDF scores using the sorted vocabulary\n",
        "    for doc in tf:\n",
        "        doc_tfidf = np.zeros(len(vocab))\n",
        "        for word, value in doc.items():\n",
        "            if word in vocab:\n",
        "                index = vocab[word]\n",
        "                doc_tfidf[index] = value * idf[word]\n",
        "        # Normalization\n",
        "        norm = np.linalg.norm(doc_tfidf)\n",
        "        if norm > 0:\n",
        "            doc_tfidf = doc_tfidf / norm\n",
        "        tfidf.append(doc_tfidf)\n",
        "\n",
        "    return np.array(tfidf)\n",
        "\n",
        "# Process documents\n",
        "processed_docs = [process_data(doc) for doc in all_docs]\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tfidf_scratch= fit_custom_tfidf(processed_docs)\n",
        "\n",
        "# Print TF-IDF from CustomTFIDF\n",
        "print(\"TF-IDF from Scratch:\")\n",
        "print(tfidf_scratch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGMdHhFggbr5",
        "outputId": "0cf4acdc-ab13-4390-fcab-ad1cc7ddbe6f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF from Scratch:\n",
            "[[0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.        ]\n",
            " [0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.        ]\n",
            " [0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.35355339 0.35355339 0.\n",
            "  0.35355339 0.35355339 0.         0.35355339 0.         0.35355339\n",
            "  0.         0.         0.         0.35355339 0.         0.\n",
            "  0.35355339]\n",
            " [0.         0.         0.         0.35355339 0.35355339 0.\n",
            "  0.35355339 0.35355339 0.         0.35355339 0.         0.35355339\n",
            "  0.         0.         0.         0.35355339 0.         0.\n",
            "  0.35355339]\n",
            " [0.         0.         0.         0.35355339 0.35355339 0.\n",
            "  0.35355339 0.35355339 0.         0.35355339 0.         0.35355339\n",
            "  0.         0.         0.         0.35355339 0.         0.\n",
            "  0.35355339]\n",
            " [0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40824829 0.40824829 0.         0.         0.         0.40824829\n",
            "  0.        ]\n",
            " [0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40824829 0.40824829 0.         0.         0.         0.40824829\n",
            "  0.        ]\n",
            " [0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40824829 0.40824829 0.         0.         0.         0.40824829\n",
            "  0.        ]]\n"
          ]
        }
      ]
    }
  ]
}